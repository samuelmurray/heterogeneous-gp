{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixed Likelihood GPLVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If project root is not in sys.path (i.e. not installed with pip), we need to add it manually\n",
    "import sys\n",
    "import os\n",
    "gpdir = os.path.abspath(\"../..\")\n",
    "sys.path.append(gpdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython import display\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import seaborn as sns\n",
    "    sns.set()\n",
    "    print(\"Using Seaborn plotting\")\n",
    "except ModuleNotFoundError:\n",
    "    print(\"Using Matplotlib plotting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gp\n",
    "from gp.util.data import oilflow, circle_data, gaussian_data\n",
    "from gp.model import MLGPLVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_data = 500\n",
    "latent_dim = 2\n",
    "y_obs, likelihoods, labels = oilflow(num_data)\n",
    "x = gp.util.PCA_reduce(y_obs, latent_dim)\n",
    "y = tf.convert_to_tensor(y_obs, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = gp.kernel.ARDRBF(variance=0.5, gamma=0.5, xdim=2, name=\"kern\")\n",
    "m = MLGPLVM(y, latent_dim, x=x, kernel=kernel, likelihoods=likelihoods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = m.loss()\n",
    "learning_rate = 5e-4\n",
    "with tf.name_scope(\"train\"):\n",
    "    with tf.variable_scope(\"\", reuse=tf.AUTO_REUSE):\n",
    "        u_vars = [tf.get_variable(\"qu/mean\"), tf.get_variable(\"qu/log_scale\")]\n",
    "        non_u_vars = [tf.get_variable(\"z\"), tf.get_variable(\"qx/mean\"), tf.get_variable(\"qx/log_std\"),\n",
    "                      tf.get_variable(\"kern/log_variance\"), tf.get_variable(\"kern/log_gamma\")]\n",
    "        train_x = tf.train.RMSPropOptimizer(learning_rate).minimize(loss, var_list=non_u_vars, name=\"train_x\")\n",
    "        train_u = tf.train.RMSPropOptimizer(learning_rate).minimize(loss, var_list=u_vars, name=\"train_u\")\n",
    "\n",
    "with tf.name_scope(\"summary\"):\n",
    "    tf.summary.scalar(\"kl_qx_px\", m.kl_qx_px(), collections=[\"training\"])\n",
    "    tf.summary.scalar(\"kl_qu_pu\", m.kl_qu_pu(), collections=[\"training\"])\n",
    "    tf.summary.scalar(\"expectation\", m.mc_expectation(), collections=[\"training\"])\n",
    "    tf.summary.scalar(\"training_loss\", loss, collections=[\"training\"])\n",
    "    tf.summary.scalar(\"kern_var\", tf.squeeze(m.kernel._variance), collections=[\"training\"])\n",
    "    #tf.summary.scalar(\"kern_gamma\", tf.squeeze(m.kern._gamma), collections=[\"training\"])\n",
    "    tf.summary.histogram(\"kern_gamma\", m.kernel._gamma, collections=[\"training\"])\n",
    "    tf.summary.histogram(\"qx_mean\", m.qx_mean, collections=[\"training\"])\n",
    "    tf.summary.histogram(\"qx_std\", m.qx_std, collections=[\"training\"])\n",
    "    tf.summary.histogram(\"z\", m.z, collections=[\"training\"])\n",
    "    tf.summary.histogram(\"qu_mean\", m.qu_mean, collections=[\"training\"])\n",
    "    tf.summary.histogram(\"qu_scale\", m.qu_scale, collections=[\"training\"])\n",
    "    merged_summary = tf.summary.merge_all(\"training\")\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_list = []\n",
    "f, (ax1, ax2) = plt.subplots(1, 2)\n",
    "n_iter = 1000\n",
    "n_print = 100\n",
    "try:\n",
    "    with tf.Session() as sess:\n",
    "        summary_writer = tf.summary.FileWriter(f\"../../log/{time.strftime('%Y%m%d%H%M%S')}\", sess.graph)\n",
    "        print(\"Initializing variables...\")\n",
    "        sess.run(init)\n",
    "        print(\"Starting training...\")\n",
    "        for i in range(n_iter):\n",
    "            sess.run(train_x)\n",
    "            sess.run(train_u)\n",
    "            if i % n_print == 0:\n",
    "                run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "                run_metadata = tf.RunMetadata()\n",
    "                train_loss, summary = sess.run([loss, merged_summary], options=run_options, run_metadata=run_metadata)\n",
    "                summary_writer.add_run_metadata(run_metadata, f\"step{i}\")\n",
    "                summary_writer.add_summary(summary, i)\n",
    "                x_mean = sess.run(m.qx_mean)\n",
    "                z = sess.run(m.z)\n",
    "                loss_list.append(train_loss)\n",
    "            \n",
    "                # Plot\n",
    "                for c in np.unique(labels):\n",
    "                    ax1.scatter(*x_mean[labels == c].T)\n",
    "                # ax1.scatter(*z.T, c=\"k\", marker=\"x\")\n",
    "                ax1.set_title(f\"Step {i} - Loss: {train_loss}\")\n",
    "                ax2.plot(loss_list)\n",
    "                display.display(f)\n",
    "                display.clear_output(wait=True)\n",
    "                ax1.cla()\n",
    "                ax2.cla()\n",
    "        x_mean = sess.run(m.qx_mean)\n",
    "        z = sess.run(m.z)\n",
    "        for c in np.unique(labels):\n",
    "            ax1.scatter(*x_mean[labels == c].T)\n",
    "        ax1.set_title(f\"Step {i} - Loss: {train_loss}\")\n",
    "        ax2.plot(loss_list)\n",
    "        display.display(f)\n",
    "        display.clear_output(wait=True)\n",
    "except KeyboardInterrupt:\n",
    "    for c in np.unique(labels):\n",
    "        ax1.scatter(*x_mean[labels == c].T)\n",
    "    ax1.set_title(f\"Step {i} - Loss: {train_loss}\")\n",
    "    ax2.plot(loss_list)\n",
    "    display.display(f)\n",
    "    display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
