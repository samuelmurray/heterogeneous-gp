{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixed Likelihood GPLVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "import os\n",
    "gpdir = os.path.abspath(\"../..\")\n",
    "sys.path.append(gpdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython import display\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gp\n",
    "from gp.util.data import oilflow\n",
    "from gp.model import MLGPLVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_data = 100\n",
    "latent_dim = 2\n",
    "y_obs, likelihoods, labels = oilflow(num_data)\n",
    "y = tf.convert_to_tensor(y_obs, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = MLGPLVM(y, latent_dim, likelihoods=likelihoods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = m.loss()\n",
    "learning_rate = 5e-4\n",
    "with tf.name_scope(\"train\"):\n",
    "    with tf.variable_scope(\"\", reuse=tf.AUTO_REUSE):\n",
    "        u_vars = [tf.get_variable(\"qu/mean\"), tf.get_variable(\"qu/log_scale\")]\n",
    "        non_u_vars = [tf.get_variable(\"z\"), tf.get_variable(\"qx/mean\"), tf.get_variable(\"qx/log_std\"),\n",
    "                      ]#tf.get_variable(\"kern/log_variance\"), tf.get_variable(\"kern/log_gamma\")]\n",
    "        train_x = tf.train.RMSPropOptimizer(learning_rate).minimize(loss, var_list=non_u_vars, name=\"train_x\")\n",
    "        train_u = tf.train.RMSPropOptimizer(learning_rate).minimize(loss, var_list=u_vars, name=\"train_u\")\n",
    "\n",
    "with tf.name_scope(\"summary\"):\n",
    "    tf.summary.scalar(\"kl_qx_px\", m.kl_qx_px(), collections=[\"training\"])\n",
    "    tf.summary.scalar(\"kl_qu_pu\", m.kl_qu_pu(), collections=[\"training\"])\n",
    "    tf.summary.scalar(\"expectation\", m.mc_expectation(), collections=[\"training\"])\n",
    "    tf.summary.scalar(\"training_loss\", loss, collections=[\"training\"])\n",
    "    tf.summary.scalar(\"kern_var\", tf.squeeze(m.kern._variance), collections=[\"training\"])\n",
    "    tf.summary.scalar(\"kern_gamma\", tf.squeeze(m.kern._gamma), collections=[\"training\"])\n",
    "    tf.summary.histogram(\"qx_mean\", m.qx_mean, collections=[\"training\"])\n",
    "    tf.summary.histogram(\"qx_std\", m.qx_std, collections=[\"training\"])\n",
    "    tf.summary.histogram(\"z\", m.z, collections=[\"training\"])\n",
    "    tf.summary.histogram(\"qu_mean\", m.qu_mean, collections=[\"training\"])\n",
    "    tf.summary.histogram(\"qu_scale\", m.qu_scale, collections=[\"training\"])\n",
    "    merged_summary = tf.summary.merge_all(\"training\")\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_list = []\n",
    "f, (ax1, ax2) = plt.subplots(1, 2)\n",
    "n_iter = 50000\n",
    "n_print = 100\n",
    "with tf.Session() as sess:\n",
    "    summary_writer = tf.summary.FileWriter(f\"../../log/{time.strftime('%Y%m%d%H%M%S')}\", sess.graph)\n",
    "    print(\"Initializing variables...\")\n",
    "    sess.run(init)\n",
    "    print(f\"Initial loss: {sess.run(loss)}\")\n",
    "    print(\"Starting training...\")\n",
    "    for i in range(n_iter):\n",
    "        sess.run(train_x)\n",
    "        sess.run(train_u)\n",
    "        if i % n_print == 0:\n",
    "            run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "            run_metadata = tf.RunMetadata()\n",
    "            train_loss, summary = sess.run([loss, merged_summary], options=run_options, run_metadata=run_metadata)\n",
    "            summary_writer.add_run_metadata(run_metadata, f\"step{i}\")\n",
    "            summary_writer.add_summary(summary, i)\n",
    "            loss_print = f\"Step {i} - Loss: {train_loss}\"\n",
    "            x_mean = sess.run(m.qx_mean)\n",
    "            z = sess.run(m.z)\n",
    "            \n",
    "            ax1.scatter(*x_mean[labels == 0].T)\n",
    "            ax1.scatter(*x_mean[labels == 1].T)\n",
    "            ax1.scatter(*x_mean[labels == 2].T)\n",
    "            #plt.scatter(z[:, 0], z[:, 1], c=\"k\", marker=\"x\")\n",
    "            ax1.set_title(loss_print)\n",
    "            \n",
    "            loss_list.append(train_loss)\n",
    "            ax2.plot(loss_list)\n",
    "            \n",
    "            display.display(f)\n",
    "            display.clear_output(wait=True)\n",
    "            ax1.cla()\n",
    "            ax2.cla()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_obs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
