{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixed Likelihood GPLVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=0\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython import display\n",
    "%matplotlib inline\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "sns.set_context(\"paper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tfgp\n",
    "from tfgp.util import data\n",
    "from tfgp.model import MLGPLVM\n",
    "print(f\"Succesfully imported package: {tfgp.__file__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_data = None\n",
    "num_classes = 36\n",
    "y_train, likelihood, labels_train = data.make_binaryalphadigits(num_data, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.loadtxt(\"../../util/binaryalphadigits_test.csv\", delimiter=\",\")\n",
    "labels_test = np.array([[i] * 9 for i in range(num_classes)]).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.zeros(y_test.shape, dtype=bool)\n",
    "frac_missing = 0.2\n",
    "num_missing = int(frac_missing * y_test.shape[1])\n",
    "idx[:, :num_missing] = 1\n",
    "_ = np.apply_along_axis(np.random.shuffle, 1, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test[idx] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_noisy = np.vstack([y_train, y_test])\n",
    "labels = np.hstack([labels_train, labels_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 10\n",
    "num_inducing = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = tfgp.kernel.ARDRBF(variance=0.5, gamma=0.5, xdim=latent_dim, name=\"kernel\")\n",
    "m = MLGPLVM(y_noisy, latent_dim, num_inducing=num_inducing, kernel=kernel, likelihood=likelihood)\n",
    "m.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.losses.get_total_loss()\n",
    "learning_rate = 1e-3\n",
    "with tf.name_scope(\"train\"):\n",
    "    trainable_vars = tf.trainable_variables()\n",
    "    optimizer = tf.train.RMSPropOptimizer(learning_rate, name=\"RMSProp\")\n",
    "    train_all = optimizer.minimize(loss, \n",
    "                                   var_list=tf.trainable_variables(),\n",
    "                                   global_step=tf.train.create_global_step(),\n",
    "                                   name=\"train\")\n",
    "with tf.name_scope(\"summary\"):\n",
    "    m.create_summaries()\n",
    "    tf.summary.scalar(\"total_loss\", loss, family=\"Loss\")\n",
    "    for reg_loss in tf.losses.get_regularization_losses():\n",
    "        tf.summary.scalar(f\"{reg_loss.name}\", reg_loss, family=\"Loss\")\n",
    "    merged_summary = tf.summary.merge_all()\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(x: np.ndarray, *, z: np.ndarray = None, gammas: np.ndarray = None, loss) -> None:\n",
    "    ax1.scatter(*x[labels<12].T, c=labels[labels<12], cmap=\"Paired\", marker=\"d\")\n",
    "    ax1.scatter(*x[np.logical_and(labels>=12, labels<24)].T, c=labels[np.logical_and(labels>=12, labels<24)], cmap=\"Paired\", marker=\"x\")\n",
    "    ax1.scatter(*x[labels>=24].T, c=labels[labels>=24], cmap=\"Paired\", marker=\"*\")\n",
    "    if z is not None:\n",
    "        ax1.scatter(*z.T, c=\"k\", marker=\"x\")\n",
    "    ax_x_min, ax_y_min = np.min(x, axis=0)\n",
    "    ax_x_max, ax_y_max = np.max(x, axis=0)\n",
    "    ax1.set_xlim(ax_x_min, ax_x_max)\n",
    "    ax1.set_ylim(ax_y_min, ax_y_max)\n",
    "    ax1.set_title(f\"Step {i}\")\n",
    "    \n",
    "    ax2.plot(*np.array(loss).T)\n",
    "    ax2.set_title(f\"Loss: {train_loss}\")\n",
    "    \n",
    "    if gammas is not None:\n",
    "        ax3.bar(range(len(gammas)), gammas, tick_label=range(len(gammas)))\n",
    "    \n",
    "    display.display(f)\n",
    "    display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = f\"../..\"\n",
    "dataset = \"alphadigits\"\n",
    "start_time = f\"{time.strftime('%Y%m%d%H%M%S')}\"\n",
    "log_dir = f\"{root_dir}/log/{dataset}/{start_time}\"\n",
    "save_dir = f\"{root_dir}/save/{dataset}/{start_time}\"\n",
    "output_dir = f\"{root_dir}/output/{dataset}/{start_time}\"\n",
    "os.makedirs(save_dir)\n",
    "os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession(config=tf.ConfigProto(log_device_placement=True))\n",
    "saver = tf.train.Saver()\n",
    "# saver.restore(sess, f\"{save_dir}/model.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))\n",
    "loss_list = []\n",
    "n_iter = 40000\n",
    "print_interval = 500\n",
    "save_interval = 5000\n",
    "try:\n",
    "    summary_writer = tf.summary.FileWriter(log_dir, sess.graph)\n",
    "    sess.run(init)\n",
    "    for i in range(n_iter):\n",
    "        sess.run(train_all)\n",
    "        if i % print_interval == 0:\n",
    "            run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "            run_metadata = tf.RunMetadata()\n",
    "            train_loss, summary = sess.run([loss, merged_summary], options=run_options, run_metadata=run_metadata)\n",
    "            summary_writer.add_run_metadata(run_metadata, f\"step_{i}\", global_step=i)\n",
    "            summary_writer.add_summary(summary, i)\n",
    "            gammas = m.kernel._gamma.eval()\n",
    "            x_mean = m.qx_mean.eval().T\n",
    "            x_mean = x_mean[:, np.argsort(gammas)[-2:]]\n",
    "            z = m.z.eval()\n",
    "            loss_list.append([i, train_loss])\n",
    "            plot(x_mean, gammas=gammas, loss=loss_list)\n",
    "            ax1.cla()\n",
    "            ax2.cla()\n",
    "            ax3.cla()\n",
    "        if i % save_interval == 0:\n",
    "            saver.save(sess, f\"{save_dir}/model.ckpt\", global_step=i)\n",
    "            np.savetxt(f\"{output_dir}/x_mean_{i}.csv\", x_mean)\n",
    "            np.savetxt(f\"{output_dir}/z_{i}.csv\", z)\n",
    "            np.savetxt(f\"{output_dir}/labels.csv\", labels)\n",
    "            plot(x_mean, gammas=gammas, loss=loss_list)\n",
    "            plt.savefig(f\"{output_dir}/fig_{i}.eps\")\n",
    "            ax1.cla()\n",
    "            ax2.cla()\n",
    "            ax3.cla()\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "finally:\n",
    "    gammas = m.kernel._gamma.eval()\n",
    "    x_mean = m.qx_mean.eval().T\n",
    "    x_mean = x_mean[:, np.argsort(gammas)[-2:]]\n",
    "    z = m.z.eval()\n",
    "    loss_list.append([i, loss.eval()])\n",
    "    plot(x_mean, gammas=gammas, loss=loss_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some more plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scale():\n",
    "    scale_0, scale_1 = m.qx_scale.eval()\n",
    "    scale_0  = np.dot(scale_0, scale_0.T)\n",
    "    scale_1  = np.dot(scale_1, scale_1.T)\n",
    "    fig, ax = plt.subplots(2, 2)\n",
    "    im_full_0 = ax[0, 0].imshow(scale_0)\n",
    "    im_off_diag_0 = ax[0, 1].imshow(scale_0 - np.diag(np.diag(scale_0)))\n",
    "    im_full_1 = ax[1, 0].imshow(scale_1)\n",
    "    im_off_diag_1 = ax[1, 1].imshow(scale_1 - np.diag(np.diag(scale_1)))\n",
    "    plt.colorbar(im_full_0, ax=ax[0, 0])\n",
    "    plt.colorbar(im_off_diag_0, ax=ax[0, 1])\n",
    "    plt.colorbar(im_full_1, ax=ax[1, 0])\n",
    "    plt.colorbar(im_off_diag_1, ax=ax[1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scale()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pca = tfgp.util.pca_reduce(y, 2)\n",
    "plt.scatter(*x_pca[labels<12].T, c=labels[labels<12], cmap=\"Paired\", marker=\"d\")\n",
    "plt.scatter(*x_pca[np.logical_and(labels>=12, labels<24)].T, c=labels[np.logical_and(labels>=12, labels<24)], cmap=\"Paired\", marker=\"x\")\n",
    "plt.scatter(*x_pca[labels>=24].T, c=labels[labels>=24], cmap=\"Paired\", marker=\"*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute 1NN error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 2\n",
    "err_mlgplvm = tfgp.util.knn_error(x_mean, labels, k)\n",
    "err_pca = tfgp.util.knn_error(x_pca, labels, k)\n",
    "print(f\"Missclasifications with MLGPLVM: {err_mlgplvm}\")\n",
    "print(f\"Missclasifications with PCA: {err_pca}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(*x_mean[labels<12].T, c=labels[labels<12], cmap=\"Paired\", marker=\"d\")\n",
    "plt.scatter(*x_mean[np.logical_and(labels>=12, labels<24)].T, c=labels[np.logical_and(labels>=12, labels<24)], cmap=\"Paired\", marker=\"x\")\n",
    "plt.scatter(*x_mean[labels>=24].T, c=labels[labels>=24], cmap=\"Paired\", marker=\"*\")\n",
    "plt.savefig(f\"{output_dir}/{dataset}.eps\", format=\"eps\", dpi=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PERPLEXITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = y_train.shape[0]\n",
    "y_true = np.loadtxt(\"../../util/binaryalphadigits_test.csv\", delimiter=\",\")\n",
    "#y_true = y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The likelihood $p(y)$ should be computed as $\\int p(y \\,|\\, l) p(l \\,|\\, \\text{model}) \\, dl$ where $l$ is the logit parameter of the Bernoulli distribution. Since this integration is hard, we approximate it as $\\frac{1}{T} \\sum_{t=1}^T p(y \\,|\\, l_t)$ where $l_t$ is sampled from $p(l \\,|\\, \\text{model})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct $p(l \\,|\\, \\text{model})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kzz = m.kernel(m.z)\n",
    "kzz_inv = tf.matrix_inverse(kzz)\n",
    "kxx = m.kernel(tf.matrix_transpose(m.qx_mean)[split:])\n",
    "kxz = m.kernel(tf.matrix_transpose(m.qx_mean)[split:], m.z)\n",
    "kzx = tf.matrix_transpose(kxz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = kxz @ kzz_inv @ tf.matrix_transpose(m.qu_mean)\n",
    "cov = kxx - kxz @ kzz_inv @ kzx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_chol = tf.cholesky(cov)# + tf.diag((tf.ones([kxx.shape[0]]) * 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = tfp.distributions.MultivariateNormalTriL(tf.matrix_transpose(mean), cov_chol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute $\\frac{1}{T} \\sum_{t=1}^T p(y_i \\,|\\, l_t)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = tf.matrix_transpose(norm.sample(100)).eval()\n",
    "ber = tfp.distributions.Bernoulli(logits=logits)\n",
    "mean_prob = np.mean(ber.prob(y_true).eval(), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute perplexity $2^{-\\sum_{i=1}^N \\log_2 p(y_i)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log2_perplexity = -np.log2(mean_prob).mean()\n",
    "perplexity = 2 ** log2_perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The log2 perplexity is {log2_perplexity} and the perplexity is {perplexity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = tf.matrix_transpose(norm.sample(1)).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_mean = np.mean(logits, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ber = tfp.distributions.Bernoulli(logits=logits_mean[idx])\n",
    "ber = tfp.distributions.Bernoulli(logits=logits_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_lik = ber.log_prob(y_true)\n",
    "mean_log_lik = log_lik.eval().mean()\n",
    "log_perplexity = -mean_log_lik\n",
    "perplexity = np.exp(log_perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The log perplexity is {log_perplexity} and the perplexity is {perplexity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = ber.prob(y_true)\n",
    "log2_prob = np.log2(prob.eval())\n",
    "mean_log2_lik = log2_prob.mean()\n",
    "log2_perplexity = -mean_log2_lik\n",
    "perplexity = 2 ** log2_perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The log2 perplexity is {log2_perplexity} and the perplexity is {perplexity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
